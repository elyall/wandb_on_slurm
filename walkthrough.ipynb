{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "efa1ad94f938f3ce299cd0ab78c04e4dc40c176ba72ef553b3abfdba6e8fcdfd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Using Weights & Biases' Sweeps module on Slurm\n",
    "\n",
    "Weights & Biases (W&B) provides a number of tools that make tracking machine learning (ML) models a lot easier. One of their most popular tools is their Sweeps module that allows you to easily perform state-of-the-art hyperparameter optimization techniques across many machines in parallel using `wandb.agent()`. Many academic researchers have access to high performance computing (HPC) clusters that utilize a Slurm job scheduler, but spinning up multiple W&B agents within a Slurm job is not straightforward. Let me walk you through how to do just that.\n",
    "\n",
    "This walkthrough will have two parts:\n",
    "\n",
    "1. Setting up your own burstable Slurm cluster on Amazon Web Services (AWS) using their [aws-plugin-for-slurm](https://github.com/aws-samples/aws-plugin-for-slurm/tree/plugin-v2).\n",
    "2. Formulating and submitting a W&B sweep on Slurm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setting up your own Slurm cluster on AWS\n",
    "Note: if using AWS's GPU instances for the first time (the P-family being the most common) you must [request a service limit increase](http://aws.amazon.com/contact-us/ec2-request). The number of nodes you request should be at least equal to the number of nodes you'll make available to your Slurm cluster (more below).\n",
    "\n",
    "AWS offers a great [plugin](https://github.com/aws-samples/aws-plugin-for-slurm/tree/plugin-v2) that greatly simplifies the process of creating your own burstable Slurm cluster. This cluster will constantly run a 'headnode' that runs the Slurm daemon (manages the job queue and spinning up resources) and cron job that pulls down unused resources. Your jobs will be run on compute nodes that are spun up when needed and torn down when not used, saving you money. The plugin also offers other cool abilities, like the ability to be an extension to an existing cluster giving you more compute power or specialized hardware when needed, or the ability to specify partitions that only use spot instances which saves you even more money.\n",
    "\n",
    "Let's get started:\n",
    "\n",
    "We will be setting up our cluster via a CloudFormation template, a yaml file that specifies all of the parameters of our cluster. But this template still requires a few inputs:\n",
    "- a Virtual Private Cloud (VPC)\n",
    "- two subnets in the VPC but in different availability zones\n",
    "- a headnode instance type\n",
    "- a compute node instance type\n",
    "- an SSH key\n",
    "\n",
    "Your AWS account comes with a default VPC and defaults subnets on every availability zone which you are more than welcome to use. If you want to set up new ones, follow these instructions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### AWS SETUP\n",
    "\n",
    "[Install the aws2 cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html).\n",
    "\n",
    "In your terminal\n",
    "\n",
    "1. Login to to the CLI: `aws configure`\n",
    "2. Create an SSH key and upload it to aws:\n",
    "3. Create a VPC: `aws2 ec2 create-vpc --cidr-block 10.0.0.0/16`\n",
    "4. Create subnet 1: `aws2 ec2 create-subnet --vpc-id [VPC_ID] --cidr-block 10.0.0.0/20 --availability-zone us-west-2a` (use `aws2 ec2 describe-vpcs` to show the VPC_ID)\n",
    "4. Create subnet 2: `aws2 ec2 create-subnet --vpc-id [VPC_ID] --cidr-block 10.0.16.0/20 --availability-zone us-west-2b`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### CloudFormation\n",
    "\n",
    "Download the CloudFormation template: `wget -q https://github.com/elyall/wandb_on_slurm/raw/main/cloudformation-template.yaml`\n",
    "\n",
    "Then edit the `cloudformation-template.yaml` to your liking. For instance you can change the maximum number of nodes allowed in your cluster at line 319, or the size of your headnode's filesystem where all of the compute nodes access your code at line 230. \n",
    "\n",
    "Next we'll setup your stack:\n",
    "\n",
    "1. Go to the [AWS Console](console.aws.amazon.com/console/home), click \"Services\" at the top left and type in or select \"CloudFormation\". \n",
    "2. Hit the \"Create Stack\" dropdown on the right side, then select \"With new resources (standard)\".\n",
    "3. Under \"Specify Template\" click \"Upload a template file\", then hit \"Choose file\" and upload the `cloudformation-template.yaml`. \n",
    "\n",
    "Hit \"Next\".\n",
    "\n",
    "![step1](imgs/cloudformation-step1.png)\n",
    "\n",
    "1. Enter a \"Stack Name\" such as \"slurm\".\n",
    "2. Select your VPC from the dropdown. (Note: the stack and VPC have to be in the same region)\n",
    "3. Select two different subnets on different availability zones with the next two dropdowns.\n",
    "4. Change the Headnode or Compute Node Instance Type if you wish to another [offered on EC2](https://aws.amazon.com/ec2/instance-types/). Make sure the value under \"Compute Node vCPUs\" matches the number of vCPUs availble in the compute node instance type.\n",
    "5. Select your SSH key from the \"Key Pair\" dropdown.\n",
    "\n",
    "Hit \"Next\".\n",
    "\n",
    "![step1](imgs/cloudformation-step2.png)\n",
    "\n",
    "Hit \"Next\" again.\n",
    "\n",
    "Agree to the acknowledgement and then select \"Create stack\".\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Login to your headnode\n",
    "1. In the AWS Console, go \"Services\" -> \"EC2\" -> \"Instances\" on the left, then check the \"headnode\" instance and select \"Connect\" near the top. \n",
    "2. Select \"SSH client\", copy the text under \"Example\", and paste it into your terminal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using Weigths & Biases on Slurm\n",
    "I'm going to demonstrate one example of how to use W&B's Sweep module on slurm. This example assumes you are using a cluster built using the aws-plugin-for-slurm, but the basic principles can be used on any slurm cluster.\n",
    "\n",
    "To start, login to your cluster then clone the repo: `git clone https://github.com/elyall/wandb_on_slurm.git`\n",
    "\n",
    "Netxt we will download the example model we will optimize, install its dependencies into a virtual environment, and login to wandb, by running the `setup.sh` script: \n",
    "```\n",
    "cd ~/wandb_on_slurm\n",
    "bash setup.sh\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#!/bin/bash\n\n# update pip\nsudo yum update -y\nsudo yum install python3-devel -y #necssary for `pip install wandb`\nsudo pip3 install --upgrade pip\n\n# create accessible logs directory\nsudo mkdir /nfs/logs\nsudo chown -R ec2-user:ec2-user /nfs/logs\n\n# create accessible code directory\nsudo mkdir /nfs/code\nsudo chown -R ec2-user:ec2-user /nfs/code\ncp ~/wandb_on_slurm/wandb_on_slurm.py /nfs/code/\ncp ~/wandb_on_slurm/start-agent.sh /nfs/code/\nchmod +x /nfs/code/start-agent.sh\ncd /nfs/code\n\n# clone example to run\ngit clone https://github.com/wandb/examples.git\n\n# create virtual environment with required dependencies\npython3 -m venv wandb-venv\nsource wandb-venv/bin/activate\npip install --upgrade -r examples/examples/keras/keras-cnn-fashion/requirements.txt\n\n# torch\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\nconda install pytorch torchvision -c pytorch\n\n# login and copy key to accessible folder\nwandb login\npython - << EOF\nimport netrc\nnrc = netrc.netrc('/home/ec2-user/.netrc')\nkey = {'work_account': nrc.authenticators('api.wandb.ai')[2]}\nimport json\nwith open('/nfs/code/keys.json', 'w') as fp:\n    json.dump(key, fp)\nEOF\nchmod 600 /nfs/code/keys.json\n\ndeactivate\ncd ~"
     ]
    }
   ],
   "source": [
    "# Here's the setup script:\n",
    "!cat setup.sh"
   ]
  },
  {
   "source": [
    "### Running a slurm job\n",
    "To run a slurm job we typically need two things:\n",
    "1. A sbatch header detailing the resources the job needs\n",
    "2. The code we want to execute\n",
    "\n",
    "#### SBATCH Header\n",
    "Slurm jobs are submitted via shell scripts that have headers specifying the resources the job needs. Here is an example header:\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --partition=aws\n",
    "#SBATCH --time=0:20:0\n",
    "#SBATCH --output=/nfs/logs/slurm-%j.log\n",
    "#SBATCH --chdir=/nfs/code/\n",
    "```\n",
    "More information on what parameters you can set [can be found here](https://slurm.schedmd.com/sbatch.html). By default most of the parameters are optional, however your cluster manager has likely made some parameters mandatory. Mandatory parametes often include:\n",
    "- `partition` - specifies what subcluster of nodes to run on.\n",
    "- `time` - specifies the maximum amount of time the job is allowed to run.\n",
    "- `qos` - what account to bill to.\n",
    "- `nodes` - the number of nodes to assign to the job.\n",
    "\n",
    "After the header is where you place your code which will run on the resources the job scheduler assigns using the header as a guide. This code can be as simple as `python run.py`, but is often more complex. Here is an example job sumbission script:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --partition=aws\n#SBATCH --time=0:20:0\n#SBATCH --output=/nfs/logs/slurm-%j.log\n#SBATCH --chdir=/nfs/code/\ndate;hostname;id;pwd;ls\n\necho 'gathering node information'\nnodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # get the node names\nnodes_array=( $nodes )\necho \"${nodes_array[@]}\"\n\necho 'activating virtual environment'\nsource wandb-venv/bin/activate\n\nconfig_yaml='/nfs/code/examples/examples/keras/keras-cnn-fashion/sweep-bayes-hyperband.yaml'\necho 'template:' $config_yaml\n\necho 'running script'\npython wandb_on_slurm.py $config_yaml \"${nodes_array[@]}\""
     ]
    }
   ],
   "source": [
    "!cat wandb_on_slurm.sbatch"
   ]
  },
  {
   "source": [
    "In this script the typical things we do that are done on any machine include:\n",
    "\n",
    "1. activate a virtual environment where the dependencies are installed\n",
    "2. specify the parameters of our sweep as defined in a yaml file\n",
    "3. run our sweep with python\n",
    "\n",
    "The unique step we have to do to take advantage of the multi-node parallelism offered by slurm, is we have to determine what nodes are assigned to the job, and then pass that list of nodes into the python script where it will spin up a W&B agent on each node.\n",
    "\n",
    "Here is what the python script looks like:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "import wandb\nimport subprocess\nimport click\nimport yaml\nimport os\nimport json\n\nif os.path.exists(\"/nfs/code/keys.json\"):\n    with open(\"/nfs/code/keys.json\") as file:\n        api_key = json.load(file)[\"work_account\"]\n        os.environ[\"WANDB_API_KEY\"] = api_key\n\n@click.command()\n@click.argument(\"config_yaml\")\n@click.argument(\"node_list\", nargs=-1)\ndef run(config_yaml, node_list):\n    project_name = \"wandb_on_slurm\"\n\n    wandb.init(project=project_name)\n    \n\n    with open(config_yaml) as file:\n        config_dict = yaml.load(file, Loader=yaml.FullLoader)\n    config_dict['program'] = '/nfs/code/examples/examples/keras/keras-cnn-fashion/train.py'\n    config_dict['parameters']['epochs']['value'] = 5\n\n    sweep_id = wandb.sweep(config_dict, project=project_name)\n    \n    sp = []\n    for node in node_list:\n        sp.append(subprocess.Popen(['srun',\n                        '--nodes=1',\n                        '--ntasks=1',\n                        '-w',\n                        node,\n                        'start-agent.sh',\n                        sweep_id,\n                        project_name]))\n    exit_codes = [p.wait() for p in sp] # wait for processes to finish\n    return exit_codes \n\n\nif __name__ == '__main__':\n    run()"
     ]
    }
   ],
   "source": [
    "!cat wandb_on_slurm.py"
   ]
  },
  {
   "source": [
    "The specialized part of this script is the for loop at the bottom where we iterate over the list of nodes and start an agent on each one using `start-agent.sh`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#!/bin/bash\n\nwandb agent $1 --project $2"
     ]
    }
   ],
   "source": [
    "!cat start-agent.sh"
   ]
  }
 ]
}